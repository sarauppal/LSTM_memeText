{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import operator\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "   \n",
    "    #remove everything after time stamp\n",
    "    text = re.sub(r'((1[0-2]|0?[1-9]):([0-5][0-9]) ?([AaPp][Mm]) ?(-) ([\\s\\S]+))', '', text)\n",
    "    #remove @mentions with user ID\n",
    "    text = re.sub(r'@[A-Za-z0-9]+','',text)\n",
    "    # Check characters to see if they are in punctuation\n",
    "    text = re.sub('\\d+','', text)\n",
    "   #remove punctuation\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    # Join the characters again to form the string.\n",
    "    combined = ''.join(nopunc)\n",
    "    \n",
    "    # convert text to lower-case\n",
    "    combined = combined.lower()\n",
    "    # remove URLs\n",
    "    combined = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', combined)\n",
    "\n",
    "    \n",
    "    # remove # but keep the hastag as word\n",
    "    combined = re.sub(r'#([^\\s]+)', r'\\1', combined)\n",
    "    # remove repeated characters\n",
    "    combined = word_tokenize(combined)\n",
    "    # remove stopwords from final word list\n",
    "    return [word for word in combined if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dir C:\\CPSC532P\\LSTM_memeText\\data\n",
    "\n",
    "class Loader4Text:\n",
    "    def __init__(self,datadir,clean_data=False):\n",
    "        self.texts_col=3\n",
    "        self.labels_col=8\n",
    "        self.clean_data=self.load_data(datadir,clean_data)\n",
    "        self.vocab,self.classes=self.build_vocab(self.clean_data)\n",
    "        self.word2id=self.get_index(self.vocab,'UNK')\n",
    "        self.class2id=self.get_index(self.classes)\n",
    "        self.train_data,self.dev_data=self.split_data(self.clean_data)\n",
    "       \n",
    "        \n",
    "        \n",
    "    def load_data(self, datadir,clean=False):\n",
    "        if (clean):\n",
    "            texts = []\n",
    "            labels = []\n",
    "            clean_texts = []\n",
    "            clean_labels = []\n",
    "            data = defaultdict(list)\n",
    "            filenames=os.listdir(datadir)\n",
    "            for f in filenames:\n",
    "                if not f.endswith('csv'):\n",
    "                    continue\n",
    "        \n",
    "                df=pd.read_csv(datadir+f)\n",
    "                texts[len(texts):]=np.array(df.iloc[:,texts_col])\n",
    "                labels[len(labels):]=np.array(df.iloc[:,label_col])\n",
    "                texts_n_labels=np.array(list(zip(texts,labels)))\n",
    "                for text,label in text_n_labels:\n",
    "                    if not (isinstance(label, type(str)) and isinstance(text, type(str))):\n",
    "                        continue\n",
    "                    clean_texts.append(preprocess(text))\n",
    "                    clean_labels.append(preprocess(label))\n",
    "        \n",
    "                clean_texts_n_labels=np.array(list(zip(clean_texts,clean_labels)))\n",
    "                pd.DataFrame(clean_texts_n_labels).to_csv(datadir+'clean/'+f, index=False,header=False)\n",
    "                \n",
    "                \n",
    "        else:\n",
    "            cleandir=datadir+'clean/'\n",
    "            filenames=os.listdir(cleandir)\n",
    "            for f in filenames:\n",
    "                clean_texts_n_labels=np.array(pd.read_csv(cleandir+f))\n",
    "             \n",
    "        \n",
    "        return clean_texts_n_labels\n",
    "    \n",
    "    def build_vocab(self,data):\n",
    "        words_set=set()\n",
    "        classes_set=set()\n",
    "        for line,label in data:\n",
    "            for word in eval(line):\n",
    "                words_set.add(word)\n",
    "            classes_set.add(label)    \n",
    "                \n",
    "                \n",
    "        \n",
    "        return words_set,classes_set\n",
    "    \n",
    "    def get_index(self,item_set, unk = None):\n",
    "        item2id = defaultdict(int)\n",
    "        if unk is not None:\n",
    "            item2id[unk] = 0\n",
    "        for item in item_set:\n",
    "            item2id[item] = len(item2id)\n",
    "            \n",
    "        return item2id    \n",
    "            \n",
    "                \n",
    "    def split_data(self,data):\n",
    "        train_split = []\n",
    "        valid_split = []\n",
    "        \n",
    "        print(\"Data Statistics\")\n",
    "        \n",
    "        fdist_class = FreqDist(self.clean_data[:,1])\n",
    "        \n",
    "        for key in fdist_class.keys():\n",
    "            print(key + \" : {} Distribution: {:.2f} %\" .format( fdist_class[key],100*fdist_class[key]/sum(fdist_class.values()) ))\n",
    "        \n",
    "        np.random.shuffle(self.clean_data)\n",
    "        train_ratio=int(len(data)*0.8)\n",
    "        \n",
    "        train_split=data[:train_ratio]\n",
    "        dev_split=data[train_ratio:]\n",
    "        \n",
    "        return train_split,dev_split\n",
    "    \n",
    "    \n",
    "\"\"\"Dataset interface provided with pytorch\"\"\"    \n",
    "\n",
    "class PaddedTensorDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping data, target and length tensors.\n",
    "\n",
    "    Each sample will be retrieved by indexing both tensors along the first\n",
    "    dimension.\n",
    "\n",
    "    Arguments:\n",
    "        data_tensor (Tensor): contains sample data.\n",
    "        target_tensor (Tensor): contains sample targets (labels).\n",
    "        length (Tensor): contains sample lengths.\n",
    "        raw_data (Any): The data that has been transformed into tensor, useful for debugging\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_tensor, target_tensor, length_tensor, raw_data):\n",
    "        assert data_tensor.size(0) == target_tensor.size(0) == length_tensor.size(0)\n",
    "        self.data_tensor = data_tensor\n",
    "        self.target_tensor = target_tensor\n",
    "        self.length_tensor = length_tensor\n",
    "        self.raw_data = raw_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data_tensor[index], self.target_tensor[index], self.length_tensor[index], self.raw_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_tensor.size(0)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(dev_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(loader.clean_data[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.clean_data[1:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
